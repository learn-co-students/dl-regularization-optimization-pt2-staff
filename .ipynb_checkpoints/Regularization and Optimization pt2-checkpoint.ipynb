{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How optimize your networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. normalized inputs will speed up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- subtracting the mean\n",
    "- normalize by dividing the variances\n",
    "- learning can be slow when inputs are unnormalized because of different scales.\n",
    "- Example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LoreDirick/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w1 = np.arange(-5, 5, 0.1)\n",
    "w2 = np.arange(-5, 5, 0.1)\n",
    "w1,w2 = np.meshgrid(w1, w2)\n",
    "J = w1**2+ w2**2\n",
    "\n",
    "surface = go.Surface(x=w1, y=w2, z=J, colorscale='Viridis')\n",
    "data = [surface]\n",
    "layout = go.Layout(\n",
    "title='Normalized inputs',\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='inputs_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LoreDirick/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w1 = np.arange(-50, 50, 0.1)\n",
    "w2 = np.arange(-5, 5, 0.1)\n",
    "w1,w2 = np.meshgrid(w1, w2)\n",
    "J = w1**2+ w2**2\n",
    "\n",
    "surface = go.Surface(x=w1, y=w2, z=J, colorscale='Viridis')\n",
    "data = [surface]\n",
    "layout = go.Layout(\n",
    "title='Unnormalized inputs',\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='inputs_unnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanishing or exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Very deep neural network. Let's assume $g(z)=z$ (so no transformation, just a linear activation function), and biases equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y = w^{[L]}w^{[L-1]}w^{[L-2]}... w^{[3]}w^{[2]}w^{[1]}x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall that $z^{[1]} =w^{[1]}x $, and that $a^{[1]}=g(z^{[1]})=z^{[1]}$\n",
    "\n",
    "similarly, $a^{[2]}=g(z^{[2]})=g(w^{[2]}a^{[1]})$\n",
    "\n",
    "Imagine 2 nodes in each layer, and w =  $\\begin{bmatrix} 1.3 & 0 \\\\ 0 & 1.3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y = w^{[L]} \\begin{bmatrix} 1.3 & 0 \\\\ 0 & 1.3 \\end{bmatrix}^{L-1}   x$\n",
    "\n",
    "even if w's slightly smaller than 1 or slightly larger, the activations will explode when there are many layers in the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your initialization wisely!\n",
    "\n",
    "The more input features feeding into layer l, the smaller we want each $w_i$ to be. Common rule of thumb: $Var(w_i)$ = 1/n or 2/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize:\n",
    "    ```w^{[l]}= np.random.randn(shape)*np.sqrt(2/n_(l-1)) ```\n",
    "    \n",
    "--> common for relu\n",
    "\n",
    "Different initializations for different activation functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens often is that gradient descent oscillates to a fairly big extent, because the derivative is bigger in the vertical direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some optimization algorithms that work faster than gradient descent:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gradient descent with momentum\n",
    "compute an exponentially weighthed average of the gradients and use that gradient instead. because asymmetric axes, you want to have slower learning on one axis, and fasted on another one.\n",
    "\n",
    "Momentum:\n",
    "compute dW and db on the current minibatch.\n",
    "\n",
    "Combute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$ and\n",
    "\n",
    "Combute $V_{db} = \\beta V_{db} + (1-\\beta)db$\n",
    "\n",
    "--> moving average for the derivatives of W and b\n",
    "\n",
    "$W:= W- \\alpha Vdw$\n",
    "\n",
    "$b:= b- \\alpha Vdb$\n",
    "\n",
    "This averages out gradient descent, and will \"dampen\" oscillations\n",
    "Generally, $\\beta=0.9$ is a good hyperparameter value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 RMSprop\n",
    "\n",
    "RMSprop: \"root mean square\" prop.\n",
    "\n",
    "Slow down learning on one direction and speed up in another one.\n",
    "\n",
    "On each iteration, use exponentially weithed average again:\n",
    "exponentially weighted average of the squares of the derivatives\n",
    "\n",
    "$S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "\n",
    "$S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "\n",
    "$W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$ and\n",
    "\n",
    "$b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "\n",
    "In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "\n",
    "Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Adam optimization algorithm\n",
    "\n",
    "\"Adaptive Moment Estimation\", basically using the first and second moment estimations.\n",
    "\n",
    "Works very well in many situations!\n",
    "\n",
    "Taking momentum and RMSprop and putting it together!\n",
    "\n",
    "Initialize:\n",
    "\n",
    "$V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "\n",
    "each iteration:\n",
    "Compute $dW, db$ using the current mini-batch\n",
    "\n",
    "$V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "\n",
    "$S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "\n",
    "Is like momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "\n",
    "\n",
    "$V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "$S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "$W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "$b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "- $\\alpha$ we need to tune\n",
    "- $\\beta_1 = 0.9$\n",
    "- $\\beta_2 = 0.999$\n",
    "- $\\epsilon = 10^{-8}$\n",
    "\n",
    "Generally, only $\\alpha$ gets tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate decreases across epochs.\n",
    "\n",
    "$\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "other methods:\n",
    "\n",
    "$\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)\n",
    "\n",
    "or\n",
    "\n",
    "$\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$\n",
    "\n",
    "or\n",
    "\n",
    "Manual decay!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've ween some optimization algorithms, let's have another look at all the hyperparameters that need tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "Things to do:\n",
    "\n",
    "- don't use a grid, because hard to say in advance which hyperparameters will be important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
